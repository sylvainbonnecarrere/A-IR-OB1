# üöÄ Orchestrator Agent - Plateforme IA Multi-Providers Avanc√©e

Une plateforme d'orchestration IA sophistiqu√©e avec support de **8 fournisseurs LLM**, gestion de **sessions persistantes**, **m√©moire automatique** et **architecture hexagonale** compl√®te.

## üåü Vue d'ensemble

Cette plateforme r√©volutionnaire offre une API RESTful unifi√©e pour orchestrer des conversations IA avec une architecture modulaire de niveau entreprise, supportant 8 grands fournisseurs LLM avec une gestion avanc√©e de sessions et une m√©moire automatique.

### ‚ú® Caract√©ristiques Principales

- **üéØ 8 Fournisseurs LLM** : OpenAI, Anthropic, Google Gemini, Mistral, Grok (xAI), Qwen, DeepSeek, Kimi K2
- **üíæ Sessions Persistantes** : Conversations continues avec IDs uniques
- **üß† M√©moire Automatique** : R√©sum√© automatique de l'historique (configurable)
- **üîÑ Orchestration Intelligente** : Bascule transparente entre providers
- **üìä M√©triques Compl√®tes** : Tracking d√©taill√© des performances et usage
- **üõ°Ô∏è S√©curit√© Renforc√©e** : Validation robuste, headers s√©curis√©s
- **üèóÔ∏è Architecture Hexagonale** : Clean Architecture avec principes SOLID
- **üìö Documentation Auto** : Swagger/OpenAPI complet
- **üîç Health Monitoring** : Surveillance en temps r√©el de tous les providers

## üèõÔ∏è Architecture Avanc√©e

### Structure Modulaire Compl√®te

```
src/
‚îú‚îÄ‚îÄ api/                           # üåê Couche API (FastAPI)
‚îÇ   ‚îî‚îÄ‚îÄ router.py                 # Endpoints RESTful complets
‚îú‚îÄ‚îÄ domain/                       # üß© Couche M√©tier
‚îÇ   ‚îú‚îÄ‚îÄ llm_service_interface.py  # Interface unifi√©e LLM
‚îÇ   ‚îú‚îÄ‚îÄ llm_service_factory.py    # Factory 8-providers
‚îÇ   ‚îî‚îÄ‚îÄ session_manager.py        # Gestionnaire sessions avanc√©
‚îú‚îÄ‚îÄ infrastructure/               # üîß Couche Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers/           # 8 Adaptateurs LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_adapter.py    # GPT-3.5/4, GPT-4o, O1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_adapter.py # Claude 3.5 Sonnet/Haiku
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_adapter.py    # Gemini 1.5 Pro/Flash
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mistral_adapter.py   # Mistral Large/Small
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grok_adapter.py      # Grok (xAI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qwen_adapter.py      # Qwen/DashScope
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek_adapter.py  # DeepSeek V3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kimi_k2_adapter.py   # Kimi K2
‚îÇ   ‚îî‚îÄ‚îÄ session_storage.py       # Persistance sessions
‚îî‚îÄ‚îÄ models/                      # üìã Mod√®les de Donn√©es
    ‚îî‚îÄ‚îÄ data_contracts.py        # Contrats Pydantic complets
```

### Gestion Avanc√©e des Sessions

```mermaid
graph LR
    A[Client Request] --> B[Session Manager]
    B --> C{Session Exists?}
    C -->|Yes| D[Load History]
    C -->|No| E[Create Session]
    D --> F[Check Summary Threshold]
    E --> F
    F --> G{Need Summary?}
    G -->|Yes| H[Auto Summarize]
    G -->|No| I[Direct Processing]
    H --> I
    I --> J[LLM Provider]
    J --> K[Update Session]
    K --> L[Response]
```

## üöÄ Installation & Configuration

### 1. Installation Standard

```bash
# Cloner le repository
git clone <repository-url>
cd orchestrator_agent_py

# Environnement virtuel Python 3.11+
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Installation des d√©pendances
pip install -r requirements.txt
```

### 2. Configuration S√©curis√©e des API Keys

Cr√©ez un fichier `.env` dans le r√©pertoire racine :

```env
# üîë Configuration des 8 Fournisseurs LLM

# OpenAI (GPT-3.5, GPT-4, GPT-4o, O1)
OPENAI_API_KEY=sk-...

# Anthropic (Claude 3.5 Sonnet/Haiku)
ANTHROPIC_API_KEY=sk-ant-api03-...

# Google Gemini (1.5 Pro/Flash)
GEMINI_API_KEY=AIzaSy...

# Mistral (Large/Small)
MISTRAL_API_KEY=...

# Grok (xAI)
GROK_API_KEY=xai-...

# Qwen/DashScope (Alibaba)
QWEN_API_KEY=sk-...

# DeepSeek V3
DEEPSEEK_API_KEY=sk-...

# Kimi K2 (Moonshot)
KIMI_K2_API_KEY=sk-...
```

### 3. Configuration Avanc√©e

```python
# Configuration dans main.py
SESSION_SUMMARIZATION_THRESHOLD = 20  # Messages avant r√©sum√© auto
MAX_SESSION_HISTORY = 50             # Limite historique
CORS_ORIGINS = ["http://localhost:3000"]  # Domaines autoris√©s
```

## üíª Utilisation Compl√®te

### D√©marrage du Serveur

```bash
python main.py
# üåü Serveur d√©marr√© sur http://localhost:8000
```

### üìñ Documentation Interactive

- **Swagger UI** : `http://localhost:8000/docs` - Interface compl√®te
- **ReDoc** : `http://localhost:8000/redoc` - Documentation avanc√©e

## üîå API Endpoints Complets

### 1. üè• Health Check Global

```http
GET /api/health
```

**R√©ponse :**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00Z",
  "version": "1.0.0",
  "providers_status": {
    "openai": true,
    "anthropic": true,
    "gemini": false,
    "mistral": true
  }
}
```

### 2. üìã Liste des Providers Disponibles

```http
GET /api/providers
```

**R√©ponse :**
```json
{
  "providers": [
    {
      "name": "openai",
      "models": ["gpt-3.5-turbo", "gpt-4", "gpt-4o", "o1-preview"],
      "is_healthy": true,
      "capabilities": ["chat", "completion", "function_calling"]
    },
    {
      "name": "anthropic", 
      "models": ["claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022"],
      "is_healthy": true,
      "capabilities": ["chat", "completion", "reasoning"]
    }
  ]
}
```

### 3. üÜï Cr√©ation de Session

```http
POST /api/sessions
```

**Corps de la requ√™te :**
```json
{
  "user_id": "user_123",
  "metadata": {
    "context": "Support technique",
    "language": "fr"
  }
}
```

**R√©ponse :**
```json
{
  "session_id": "sess_abc123def456",
  "created_at": "2024-01-01T12:00:00Z",
  "status": "active",
  "message_count": 0,
  "summary": null
}
```

### 4. üéØ Orchestration avec Session

```http
POST /api/orchestrate
```

**Corps de la requ√™te :**
```json
{
  "message": "Explique-moi l'intelligence artificielle en termes simples",
  "agent_config": {
    "provider": "anthropic",
    "model": "claude-3-5-sonnet-20241022",
    "temperature": 0.7,
    "max_tokens": 1000
  },
  "session_id": "sess_abc123def456"
}
```

**R√©ponse :**
```json
{
  "response": "L'intelligence artificielle est comme donner √† un ordinateur la capacit√© de \"r√©fl√©chir\" et d'apprendre...",
  "agent_used": "anthropic",
  "model_used": "claude-3-5-sonnet-20241022",
  "session_id": "sess_abc123def456",
  "message_count": 1,
  "timestamp": "2024-01-01T12:00:00Z",
  "execution_time": 2.34,
  "metadata": {
    "tokens_used": 456,
    "cost_estimate": 0.023,
    "was_summarized": false
  }
}
```

### 5. üìä M√©triques de Session

```http
GET /api/sessions/{session_id}/metrics
```

**R√©ponse :**
```json
{
  "session_id": "sess_abc123def456",
  "message_count": 15,
  "providers_used": ["openai", "anthropic", "gemini"],
  "total_tokens": 12500,
  "total_cost": 1.25,
  "average_response_time": 1.8,
  "last_summary_at": "2024-01-01T11:30:00Z",
  "created_at": "2024-01-01T10:00:00Z"
}
```

### 6. üìú Historique de Session

```http
GET /api/sessions/{session_id}/history?limit=10&offset=0
```

**R√©ponse :**
```json
{
  "session_id": "sess_abc123def456",
  "messages": [
    {
      "id": "msg_001",
      "role": "user",
      "content": "Explique-moi l'IA",
      "timestamp": "2024-01-01T10:05:00Z"
    },
    {
      "id": "msg_002", 
      "role": "assistant",
      "content": "L'intelligence artificielle...",
      "provider": "anthropic",
      "model": "claude-3-5-sonnet-20241022",
      "timestamp": "2024-01-01T10:05:02Z"
    }
  ],
  "summary": "Discussion sur les concepts de base de l'intelligence artificielle...",
  "has_more": true
}
```

## üéÆ Exemples d'Usage Avanc√©s

### Scenario 1: Conversation Multi-Provider

```python
import requests

base_url = "http://localhost:8000/api"

# 1. Cr√©er une session
session = requests.post(f"{base_url}/sessions", json={
    "user_id": "data_scientist_01",
    "metadata": {"project": "ML Pipeline"}
}).json()

session_id = session["session_id"]

# 2. Question avec GPT-4
response1 = requests.post(f"{base_url}/orchestrate", json={
    "message": "Aide-moi √† concevoir un pipeline ML pour la classification d'images",
    "agent_config": {
        "provider": "openai",
        "model": "gpt-4",
        "temperature": 0.3
    },
    "session_id": session_id
}).json()

# 3. Approfondissement avec Claude
response2 = requests.post(f"{base_url}/orchestrate", json={
    "message": "Peux-tu d√©tailler les √©tapes de pr√©paration des donn√©es?",
    "agent_config": {
        "provider": "anthropic", 
        "model": "claude-3-5-sonnet-20241022",
        "temperature": 0.5
    },
    "session_id": session_id
}).json()

# 4. Code avec DeepSeek
response3 = requests.post(f"{base_url}/orchestrate", json={
    "message": "√âcris-moi le code Python pour impl√©menter ce pipeline",
    "agent_config": {
        "provider": "deepseek",
        "model": "deepseek-chat",
        "temperature": 0.1
    },
    "session_id": session_id
}).json()
```

### Scenario 2: Session avec R√©sum√© Automatique

```python
# Configuration d'une session longue qui d√©clenche le r√©sum√© automatique
for i in range(25):  # D√©passe le seuil de 20 messages
    response = requests.post(f"{base_url}/orchestrate", json={
        "message": f"Question {i+1} sur l'apprentissage machine",
        "agent_config": {"provider": "gemini", "model": "gemini-1.5-pro"},
        "session_id": session_id
    }).json()
    
    if response.get("metadata", {}).get("was_summarized"):
        print(f"‚úÖ R√©sum√© automatique d√©clench√© au message {i+1}")
        break
```

### Scenario 3: Monitoring de Performance

```python
# Surveiller les performances de tous les providers
providers = requests.get(f"{base_url}/providers").json()["providers"]

for provider in providers:
    if provider["is_healthy"]:
        # Test de latence
        start_time = time.time()
        response = requests.post(f"{base_url}/orchestrate", json={
            "message": "Test de performance",
            "agent_config": {"provider": provider["name"]},
            "session_id": session_id
        })
        latency = time.time() - start_time
        print(f"{provider['name']}: {latency:.2f}s")
```

## üõ°Ô∏è S√©curit√© & Bonnes Pratiques

### Configuration de Production

```python
# main.py - Configuration s√©curis√©e
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://votre-domaine.com"],  # ‚ö†Ô∏è Jamais "*" en prod
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["Authorization", "Content-Type"],
)

# Headers de s√©curit√© renforc√©s
app.add_middleware(SecurityHeadersMiddleware)
```

### Variables d'Environnement S√©curis√©es

```bash
# üîí Bonnes pratiques pour les API keys

# ‚úÖ Utiliser un gestionnaire de secrets (AWS Secrets Manager, Azure Key Vault)
# ‚úÖ Rotation r√©guli√®re des cl√©s
# ‚úÖ Monitoring des usages
# ‚úÖ Restrictions par IP si possible

# Format de validation automatique
OPENAI_API_KEY=sk-[a-zA-Z0-9]{48}
ANTHROPIC_API_KEY=sk-ant-api03-[a-zA-Z0-9-_]{95}
```

### Monitoring de S√©curit√©

```python
# Alertes de s√©curit√© configurables
SECURITY_ALERTS = {
    "max_requests_per_minute": 100,
    "suspicious_patterns": ["admin", "root", "../"],
    "blocked_countries": ["XX", "YY"],
    "rate_limit_per_session": 50
}
```

## üìä M√©triques & Performance

### Dashboard de Monitoring

```python
# M√©triques temps r√©el disponibles via /api/metrics
{
    "system": {
        "uptime": "5d 12h 34m",
        "requests_total": 15420,
        "errors_rate": 0.02
    },
    "providers": {
        "openai": {"requests": 5420, "avg_latency": 1.2, "success_rate": 0.99},
        "anthropic": {"requests": 4100, "avg_latency": 1.8, "success_rate": 0.98},
        "gemini": {"requests": 3200, "avg_latency": 2.1, "success_rate": 0.97}
    },
    "sessions": {
        "active": 142,
        "total_created": 1520,
        "avg_duration": "00:23:45"
    }
}
```

## üß™ Tests Complets

### Suite de Tests Automatis√©s

```bash
# Tests complets avec couverture
python -m pytest tests/ --cov=src --cov-report=html

# Tests par cat√©gorie
python -m pytest tests/test_session_management.py  # Sessions
python -m pytest tests/test_llm_providers.py       # Providers
python -m pytest tests/test_security.py           # S√©curit√©
python -m pytest tests/test_performance.py        # Performance

# Tests d'int√©gration E2E
python -m pytest tests/test_integration_e2e.py -v
```

### R√©sultats de Tests (Jalon 3.5)

```
‚úÖ 6/6 tests pass√©s (100%)
‚îú‚îÄ‚îÄ test_session_creation_and_retrieval ‚úÖ
‚îú‚îÄ‚îÄ test_orchestration_with_session ‚úÖ  
‚îú‚îÄ‚îÄ test_session_persistence ‚úÖ
‚îú‚îÄ‚îÄ test_automatic_summarization ‚úÖ
‚îú‚îÄ‚îÄ test_session_metrics_calculation ‚úÖ
‚îî‚îÄ‚îÄ test_session_history_retrieval ‚úÖ

Coverage: 94% (target: >90%)
```

## üöÄ D√©ploiement Production

### Docker Containeris√©

```dockerfile
FROM python:3.11-slim

# S√©curit√© renforc√©e
RUN useradd -m -u 1000 appuser
WORKDIR /app

# Installation optimis√©e
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Code application
COPY --chown=appuser:appuser . .
USER appuser

# Health check int√©gr√©
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: orchestrator-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: orchestrator-agent
  template:
    metadata:
      labels:
        app: orchestrator-agent
    spec:
      containers:
      - name: api
        image: orchestrator-agent:latest
        ports:
        - containerPort: 8000
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: llm-secrets
              key: openai
        resources:
          limits:
            memory: "1Gi"
            cpu: "500m"
          requests:
            memory: "512Mi"
            cpu: "250m"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
```

## ü§ù Contribution & D√©veloppement

### Ajout d'un Nouveau Provider

```python
# 1. Cr√©er l'adaptateur
# src/infrastructure/llm_providers/nouveau_provider.py

class NouveauProviderAdapter(LLMServiceInterface):
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("NOUVEAU_API_KEY")
        self.client = NouveauClient(api_key=self.api_key) if self.api_key else None
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """Impl√©mentation sp√©cifique du nouveau provider"""
        if not self.client:
            raise ValueError("Client non initialis√©")
        
        response = await self.client.create_completion(
            prompt=prompt,
            **kwargs
        )
        return response.content
    
    def is_healthy(self) -> bool:
        return self.client is not None and self.api_key is not None
    
    def get_models(self) -> List[str]:
        return ["nouveau-model-v1", "nouveau-model-v2"]

# 2. Enregistrer dans la factory
# src/domain/llm_service_factory.py
def _register_providers(self):
    providers = {
        # ... providers existants
        "nouveau": NouveauProviderAdapter
    }

# 3. Ajouter les tests
# tests/test_nouveau_provider.py
def test_nouveau_provider_integration():
    adapter = NouveauProviderAdapter()
    assert adapter.is_healthy() == (adapter.api_key is not None)
```

### Standards de D√©veloppement

- **Code Style** : Black + Flake8
- **Type Hints** : mypy strict
- **Docstrings** : Google format
- **Tests** : pytest + coverage >90%
- **CI/CD** : GitHub Actions
- **Security** : bandit + safety

## üìà Roadmap & √âvolutions

### Prochaines Fonctionnalit√©s

- **üîÑ Auto-failover** : Basculement automatique entre providers
- **üéØ Smart routing** : S√©lection automatique du meilleur provider
- **üì± WebSocket** : Communication temps r√©el
- **üîç Advanced analytics** : ML pour optimisation des performances
- **üåç Multi-language** : Support i18n complet
- **üîê OAuth2** : Authentication avanc√©e

---

## üìÑ Licence & Support

**Licence :** MIT License  
**Support :** Issues GitHub  
**Documentation :** Wiki complet disponible  
**Community :** Discord/Slack channels

---

> üöÄ **Orchestrator Agent** - La plateforme IA la plus avanc√©e pour l'orchestration multi-providers avec sessions persistantes et m√©moire automatique. Pr√™t pour la production, s√©curis√© et hautement performant.