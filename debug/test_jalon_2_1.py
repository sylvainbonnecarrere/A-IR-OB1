"""
Script de test et validation pour le Jalon 2.1 - OpenAIAdapter.

Ce script valide que :
1. L'OpenAIAdapter peut Ãªtre instanciÃ© correctement
2. Les mÃ©thodes basiques retournent les types attendus
3. La mÃ©thode chat_completion fonctionne avec une configuration minimale
4. La conversion des formats fonctionne correctement

IMPORTANT : Pour exÃ©cuter ce test, vous devez :
1. DÃ©finir la variable d'environnement OPENAI_API_KEY avec votre clÃ© API OpenAI
2. Avoir des crÃ©dits disponibles sur votre compte OpenAI

Usage: python debug/test_jalon_2_1.py
"""

import asyncio
import os
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire source au PYTHONPATH
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

try:
    from infrastructure.llm_providers.openai_adapter import OpenAIAdapter
    from models.data_contracts import AgentConfig, ChatMessage, OrchestrationResponse
except ImportError as e:
    print(f"âŒ Erreur d'import : {e}")
    print("Assurez-vous que le script est exÃ©cutÃ© depuis le rÃ©pertoire orchestrator_agent_py")
    sys.exit(1)


async def test_openai_adapter_instantiation():
    """
    Teste l'instanciation de l'OpenAIAdapter.
    """
    print("ğŸ” Test 1: Instanciation de l'OpenAIAdapter")
    
    try:
        # Test sans clÃ© API (doit Ã©chouer)
        if os.getenv("OPENAI_API_KEY"):
            # Sauvegarde temporaire
            temp_key = os.environ["OPENAI_API_KEY"]
            del os.environ["OPENAI_API_KEY"]
            
            try:
                adapter = OpenAIAdapter()
                print("âŒ FAIL: L'adapter devrait Ã©chouer sans clÃ© API")
                return False
            except ValueError as expected:
                print("âœ… PASS: Erreur correctement levÃ©e sans clÃ© API")
            
            # Restauration de la clÃ©
            os.environ["OPENAI_API_KEY"] = temp_key
        
        # Test avec clÃ© API
        adapter = OpenAIAdapter(model="gpt-3.5-turbo")
        print("âœ… PASS: OpenAIAdapter instanciÃ© avec succÃ¨s")
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors de l'instanciation : {e}")
        return False


async def test_basic_methods():
    """
    Teste les mÃ©thodes basiques de l'adaptateur.
    """
    print("\nğŸ” Test 2: MÃ©thodes basiques")
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo")
        
        # Test get_model_name
        model_name = await adapter.get_model_name()
        if model_name == "gpt-3.5-turbo":
            print("âœ… PASS: get_model_name retourne le bon modÃ¨le")
        else:
            print(f"âŒ FAIL: get_model_name retourne {model_name}, attendu gpt-3.5-turbo")
            return False
        
        # Test get_supported_tools
        tools = await adapter.get_supported_tools()
        if isinstance(tools, list) and len(tools) > 0:
            print(f"âœ… PASS: get_supported_tools retourne une liste : {tools}")
        else:
            print(f"âŒ FAIL: get_supported_tools retourne : {tools}")
            return False
        
        # Test format_tools_for_llm
        formatted = await adapter.format_tools_for_llm(["get_time"])
        if formatted is None:  # Attendu pour ce jalon
            print("âœ… PASS: format_tools_for_llm retourne None (placeholder)")
        else:
            print(f"âŒ FAIL: format_tools_for_llm retourne : {formatted}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test des mÃ©thodes basiques : {e}")
        return False


async def test_chat_completion_mock():
    """
    Teste la mÃ©thode chat_completion avec une configuration mock.
    
    Note: Ce test ne fait PAS d'appel rÃ©el Ã  l'API OpenAI pour Ã©viter les coÃ»ts.
    Il teste uniquement la logique de conversion des formats.
    """
    print("\nğŸ” Test 3: Logique de conversion chat_completion (mock)")
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo")
        
        # Configuration d'agent de test
        config = AgentConfig(
            agent_id="test-agent",
            provider="openai",
            model="gpt-3.5-turbo",
            system_prompt="Tu es un assistant utile et concis.",
            tools=[],
            temperature=0.7,
            max_tokens=100
        )
        
        # Historique de test
        history = [
            ChatMessage(
                role="user",
                content="Bonjour, comment Ã§a va ?"
            )
        ]
        
        # Test de la conversion vers le format OpenAI
        messages = adapter._convert_history_to_openai_format(config, history)
        
        expected_structure = [
            {"role": "system", "content": "Tu es un assistant utile et concis."},
            {"role": "user", "content": "Bonjour, comment Ã§a va ?"}
        ]
        
        if messages == expected_structure:
            print("âœ… PASS: Conversion vers format OpenAI correcte")
        else:
            print(f"âŒ FAIL: Conversion incorrecte")
            print(f"   Attendu: {expected_structure}")
            print(f"   Obtenu:  {messages}")
            return False
        
        print("âœ… PASS: Logique de conversion validÃ©e")
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test de conversion : {e}")
        return False


async def test_chat_completion_real():
    """
    Teste un appel rÃ©el Ã  l'API OpenAI (optionnel).
    
    Ce test est conditionnel et ne s'exÃ©cute que si :
    1. La clÃ© API est disponible
    2. L'utilisateur confirme qu'il veut faire un appel rÃ©el
    """
    print("\nğŸ” Test 4: Appel rÃ©el Ã  l'API OpenAI (optionnel)")
    
    if not os.getenv("OPENAI_API_KEY"):
        print("â­ï¸  SKIP: Pas de clÃ© API OpenAI disponible")
        return True
    
    # Demande de confirmation (simulÃ©e pour le script automatique)
    # Dans un vrai test interactif, on demanderait Ã  l'utilisateur
    perform_real_call = False  # Changez en True pour tester avec un vrai appel
    
    if not perform_real_call:
        print("â­ï¸  SKIP: Test d'appel rÃ©el dÃ©sactivÃ© (changez perform_real_call=True pour l'activer)")
        return True
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo")
        
        # Configuration simple
        config = AgentConfig(
            agent_id="test-agent",
            provider="openai",
            model="gpt-3.5-turbo",
            system_prompt="Tu es un assistant qui rÃ©pond en une phrase courte.",
            tools=[],
            temperature=0.1,  # DÃ©terministe
            max_tokens=50     # LimitÃ© pour rÃ©duire les coÃ»ts
        )
        
        # Historique simple
        history = [
            ChatMessage(role="user", content="Dis juste 'test rÃ©ussi'")
        ]
        
        print("ğŸš€ Appel Ã  l'API OpenAI en cours...")
        response = await adapter.chat_completion(config, history)
        
        # Validation du type de retour
        if isinstance(response, OrchestrationResponse):
            print("âœ… PASS: Type de retour correct (OrchestrationResponse)")
        else:
            print(f"âŒ FAIL: Type de retour incorrect : {type(response)}")
            return False
        
        # Validation du contenu
        if response.response_message.role == "assistant":
            print("âœ… PASS: RÃ´le de la rÃ©ponse correct")
        else:
            print(f"âŒ FAIL: RÃ´le incorrect : {response.response_message.role}")
            return False
        
        if response.response_message.content:
            print(f"âœ… PASS: RÃ©ponse reÃ§ue : '{response.response_message.content[:50]}...'")
        else:
            print("âŒ FAIL: Contenu de rÃ©ponse vide")
            return False
        
        # Validation des mÃ©tadonnÃ©es
        if "model_used" in response.execution_metadata:
            print("âœ… PASS: MÃ©tadonnÃ©es d'exÃ©cution prÃ©sentes")
        else:
            print("âŒ FAIL: MÃ©tadonnÃ©es manquantes")
            return False
        
        print("âœ… PASS: Appel rÃ©el Ã  l'API OpenAI rÃ©ussi")
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors de l'appel rÃ©el : {e}")
        return False


async def run_all_tests():
    """
    ExÃ©cute tous les tests de validation du Jalon 2.1.
    """
    print("ğŸš€ Validation du Jalon 2.1 - OpenAIAdapter")
    print("=" * 60)
    
    tests = [
        test_openai_adapter_instantiation,
        test_basic_methods,
        test_chat_completion_mock,
        test_chat_completion_real
    ]
    
    results = []
    
    for test_func in tests:
        result = await test_func()
        results.append(result)
    
    # RÃ©sumÃ©
    print("\n" + "=" * 60)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
    
    passed = sum(1 for r in results if r)
    total = len(results)
    
    print(f"âœ… Tests rÃ©ussis: {passed}/{total}")
    
    if passed >= 3:  # Les 3 premiers tests sont obligatoires
        print("ğŸ‰ JALON 2.1 VALIDÃ‰ - L'OpenAIAdapter fonctionne correctement!")
        print("\nğŸ“‹ FonctionnalitÃ©s validÃ©es :")
        print("   âœ… Instanciation avec gestion des erreurs")
        print("   âœ… MÃ©thodes basiques implÃ©mentÃ©es")
        print("   âœ… Logique de conversion des formats")
        print("   âœ… Architecture respectant l'interface LLMService")
        
        if results[3]:  # Si le test rÃ©el a aussi rÃ©ussi
            print("   âœ… Appel rÃ©el Ã  l'API OpenAI fonctionnel")
        
        print("\nğŸš€ PrÃªt pour le Jalon 2.2 (FaÃ§ade et Injection de DÃ©pendances)")
    else:
        print("âš ï¸  Certains tests obligatoires ont Ã©chouÃ©.")
        print("   VÃ©rifiez l'implÃ©mentation de l'OpenAIAdapter.")
    
    print("\nğŸ“ Note :")
    print("   Pour tester avec un vrai appel API, dÃ©finissez OPENAI_API_KEY")
    print("   et changez perform_real_call=True dans test_chat_completion_real()")


if __name__ == "__main__":
    asyncio.run(run_all_tests())