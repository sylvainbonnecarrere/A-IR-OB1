"""
Script de test unitaire pour le Jalon 2.1 - OpenAIAdapter (sans API rÃ©elle).

Ce script valide que :
1. L'OpenAIAdapter peut Ãªtre instanciÃ© avec une clÃ© factice
2. Les mÃ©thodes basiques retournent les types attendus
3. La logique de conversion des formats fonctionne correctement
4. L'architecture respecte l'interface LLMService

Ce test ne fait PAS d'appel rÃ©el Ã  l'API OpenAI pour Ã©viter les coÃ»ts
et permet de valider la logique indÃ©pendamment de l'API externe.

Usage: python debug/test_jalon_2_1_offline.py
"""

import asyncio
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire source au PYTHONPATH
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

try:
    from infrastructure.llm_providers.openai_adapter import OpenAIAdapter
    from models.data_contracts import AgentConfig, ChatMessage, OrchestrationResponse
    from domain.llm_service_interface import LLMService
except ImportError as e:
    print(f"âŒ Erreur d'import : {e}")
    print("Assurez-vous que le script est exÃ©cutÃ© depuis le rÃ©pertoire orchestrator_agent_py")
    sys.exit(1)


async def test_interface_compliance():
    """
    Teste que l'OpenAIAdapter respecte l'interface LLMService.
    """
    print("ğŸ” Test 1: ConformitÃ© Ã  l'interface LLMService")
    
    try:
        # Test avec une clÃ© factice
        adapter = OpenAIAdapter(model="gpt-3.5-turbo", api_key="fake-key-for-testing")
        
        # VÃ©rification que c'est bien une instance de LLMService
        if isinstance(adapter, LLMService):
            print("âœ… PASS: OpenAIAdapter hÃ©rite correctement de LLMService")
        else:
            print("âŒ FAIL: OpenAIAdapter n'hÃ©rite pas de LLMService")
            return False
        
        # VÃ©rification que les mÃ©thodes abstraites sont implÃ©mentÃ©es
        required_methods = [
            'chat_completion',
            'get_supported_tools', 
            'get_model_name',
            'format_tools_for_llm'
        ]
        
        for method_name in required_methods:
            if hasattr(adapter, method_name) and callable(getattr(adapter, method_name)):
                print(f"âœ… PASS: MÃ©thode {method_name} implÃ©mentÃ©e")
            else:
                print(f"âŒ FAIL: MÃ©thode {method_name} manquante")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test d'interface : {e}")
        return False


async def test_basic_methods_offline():
    """
    Teste les mÃ©thodes basiques sans appel API.
    """
    print("\nğŸ” Test 2: MÃ©thodes basiques (offline)")
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo", api_key="fake-key")
        
        # Test get_model_name
        model_name = await adapter.get_model_name()
        if model_name == "gpt-3.5-turbo":
            print("âœ… PASS: get_model_name retourne le bon modÃ¨le")
        else:
            print(f"âŒ FAIL: get_model_name retourne {model_name}, attendu gpt-3.5-turbo")
            return False
        
        # Test get_supported_tools
        tools = await adapter.get_supported_tools()
        if isinstance(tools, list) and len(tools) > 0:
            print(f"âœ… PASS: get_supported_tools retourne une liste : {tools}")
        else:
            print(f"âŒ FAIL: get_supported_tools retourne : {tools}")
            return False
        
        # Test format_tools_for_llm
        formatted = await adapter.format_tools_for_llm(["get_time"])
        if formatted is None:  # Attendu pour ce jalon
            print("âœ… PASS: format_tools_for_llm retourne None (placeholder)")
        else:
            print(f"âŒ FAIL: format_tools_for_llm retourne : {formatted}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test des mÃ©thodes basiques : {e}")
        return False


async def test_format_conversion():
    """
    Teste la logique de conversion des formats.
    """
    print("\nğŸ” Test 3: Logique de conversion des formats")
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo", api_key="fake-key")
        
        # Configuration d'agent de test
        config = AgentConfig(
            agent_id="test-agent",
            provider="openai",
            model="gpt-3.5-turbo",
            system_prompt="Tu es un assistant utile et concis.",
            tools=["get_time"],
            temperature=0.7,
            max_tokens=100
        )
        
        # Historique de test complexe
        history = [
            ChatMessage(role="user", content="Bonjour, comment Ã§a va ?"),
            ChatMessage(role="assistant", content="Je vais bien, merci !"),
            ChatMessage(role="user", content="Quelle heure est-il ?")
        ]
        
        # Test de la conversion vers le format OpenAI
        messages = adapter._convert_history_to_openai_format(config, history)
        
        # VÃ©rifications
        if len(messages) == 4:  # 1 system + 3 messages
            print("âœ… PASS: Bon nombre de messages convertis")
        else:
            print(f"âŒ FAIL: {len(messages)} messages, attendu 4")
            return False
        
        if messages[0]["role"] == "system" and messages[0]["content"] == config.system_prompt:
            print("âœ… PASS: Prompt systÃ¨me correctement placÃ©")
        else:
            print("âŒ FAIL: Prompt systÃ¨me incorrect")
            return False
        
        if messages[1]["role"] == "user" and messages[1]["content"] == "Bonjour, comment Ã§a va ?":
            print("âœ… PASS: Premier message utilisateur correct")
        else:
            print("âŒ FAIL: Premier message utilisateur incorrect")
            return False
        
        if messages[2]["role"] == "assistant" and messages[2]["content"] == "Je vais bien, merci !":
            print("âœ… PASS: Message assistant correct")
        else:
            print("âŒ FAIL: Message assistant incorrect")
            return False
        
        print("âœ… PASS: Logique de conversion validÃ©e")
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test de conversion : {e}")
        return False


async def test_data_models_integration():
    """
    Teste l'intÃ©gration avec les modÃ¨les de donnÃ©es Pydantic.
    """
    print("\nğŸ” Test 4: IntÃ©gration avec les modÃ¨les de donnÃ©es")
    
    try:
        adapter = OpenAIAdapter(model="gpt-3.5-turbo", api_key="fake-key")
        
        # Test de crÃ©ation d'un ChatMessage
        message = ChatMessage(
            role="assistant",
            content="Test message",
            metadata={"source": "openai", "model": "gpt-3.5-turbo"}
        )
        
        if message.role == "assistant" and message.content == "Test message":
            print("âœ… PASS: ChatMessage crÃ©Ã© correctement")
        else:
            print("âŒ FAIL: ChatMessage incorrect")
            return False
        
        # Test de validation Pydantic
        try:
            invalid_config = AgentConfig(
                agent_id="",  # Invalide : trop court
                provider="openai",
                model="gpt-3.5-turbo",
                system_prompt="Test"
            )
            print("âŒ FAIL: Validation Pydantic non appliquÃ©e")
            return False
        except Exception:
            print("âœ… PASS: Validation Pydantic fonctionne")
        
        # Test de configuration valide
        valid_config = AgentConfig(
            agent_id="test-agent-123",
            provider="openai",
            model="gpt-3.5-turbo",
            system_prompt="Tu es un assistant de test pour valider l'architecture.",
            tools=["get_time", "calculate"],
            temperature=0.5,
            max_tokens=500
        )
        
        if valid_config.temperature == 0.5 and len(valid_config.tools) == 2:
            print("âœ… PASS: AgentConfig valide crÃ©Ã©e")
        else:
            print("âŒ FAIL: AgentConfig invalide")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test d'intÃ©gration : {e}")
        return False


async def test_error_handling():
    """
    Teste la gestion des erreurs.
    """
    print("\nğŸ” Test 5: Gestion des erreurs")
    
    try:
        # Test sans clÃ© API
        try:
            adapter = OpenAIAdapter(model="gpt-3.5-turbo")  # Pas de clÃ© API
            print("âŒ FAIL: Devrait lever une erreur sans clÃ© API")
            return False
        except ValueError as e:
            if "ClÃ© API OpenAI manquante" in str(e):
                print("âœ… PASS: Erreur correcte levÃ©e sans clÃ© API")
            else:
                print(f"âŒ FAIL: Mauvais message d'erreur : {e}")
                return False
        
        # Test avec clÃ© valide
        adapter = OpenAIAdapter(model="gpt-4", api_key="valid-fake-key")
        model = await adapter.get_model_name()
        if model == "gpt-4":
            print("âœ… PASS: ModÃ¨le personnalisÃ© acceptÃ©")
        else:
            print("âŒ FAIL: ModÃ¨le personnalisÃ© rejetÃ©")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ FAIL: Erreur lors du test de gestion d'erreurs : {e}")
        return False


async def run_all_tests():
    """
    ExÃ©cute tous les tests offline de validation du Jalon 2.1.
    """
    print("ğŸš€ Validation du Jalon 2.1 - OpenAIAdapter (Tests Offline)")
    print("=" * 70)
    
    tests = [
        test_interface_compliance,
        test_basic_methods_offline,
        test_format_conversion,
        test_data_models_integration,
        test_error_handling
    ]
    
    results = []
    
    for test_func in tests:
        result = await test_func()
        results.append(result)
    
    # RÃ©sumÃ©
    print("\n" + "=" * 70)
    print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS OFFLINE")
    
    passed = sum(1 for r in results if r)
    total = len(results)
    
    print(f"âœ… Tests rÃ©ussis: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ JALON 2.1 VALIDÃ‰ - L'OpenAIAdapter fonctionne correctement!")
        print("\nğŸ“‹ FonctionnalitÃ©s validÃ©es :")
        print("   âœ… ConformitÃ© Ã  l'interface LLMService")
        print("   âœ… MÃ©thodes abstraites correctement implÃ©mentÃ©es")
        print("   âœ… Logique de conversion des formats")
        print("   âœ… IntÃ©gration avec les modÃ¨les Pydantic")
        print("   âœ… Gestion des erreurs appropriÃ©e")
        print("   âœ… Architecture respectant les principes SOLID")
        
        print("\nğŸ—ï¸  Architecture validÃ©e :")
        print("   âœ… OCP : Extension sans modification de l'interface")
        print("   âœ… DIP : DÃ©pendance vers l'abstraction LLMService")
        print("   âœ… SRP : ResponsabilitÃ© unique = adaptateur OpenAI")
        print("   âœ… ISP : Interface cohÃ©rente et spÃ©cialisÃ©e")
        
        print("\nğŸš€ PrÃªt pour le Jalon 2.2 (FaÃ§ade et Injection de DÃ©pendances)")
    else:
        print("âš ï¸  Certains tests ont Ã©chouÃ©.")
        print("   VÃ©rifiez l'implÃ©mentation de l'OpenAIAdapter.")
    
    print("\nğŸ“ Notes importantes :")
    print("   âœ… Tests offline rÃ©ussis - logique validÃ©e")
    print("   ğŸ’¡ Pour tester avec l'API rÃ©elle, utilisez test_jalon_2_1.py avec OPENAI_API_KEY")


if __name__ == "__main__":
    asyncio.run(run_all_tests())